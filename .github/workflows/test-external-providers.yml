name: Test External Providers

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
    paths:
      - 'llama_stack/**'
      - 'tests/external-provider/**'
      - 'uv.lock'
      - 'pyproject.toml'
      - 'requirements.txt'

jobs:
  setup:
    uses: ./.github/workflows/setup-ollama.yml
    with:
      model: llama3.2:3b-instruct-fp16

  test:
    needs: setup
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v5
        with:
          python-version: "3.10"

      - name: Restore Ollama models cache
        uses: actions/cache@v4
        with:
          path: ~/.ollama/models
          key: ollama-models-llama3.2:3b-instruct-fp16-${{ hashFiles('llama_stack/templates/ollama/**') }}
          fail-on-cache-miss: true

      - name: Start Ollama service
        run: |
          # Just start the Ollama service daemon
          systemctl --user start ollama || true
          ollama serve &

      - name: Set Up Environment and Install Dependencies
        run: |
          uv sync --extra dev --extra test
          uv pip install ollama faiss-cpu
          uv pip install -e .

      - name: Run Test
        env:
          INFERENCE_MODEL: "meta-llama/Llama-3.2-3B-Instruct"
        run: |
          llama stack run tests/external-provider/llama-stack-provider-ollama/run.yaml \
            --image-type venv \
            -p custom_ollama=tests/external-provider/llama-stack-provider-ollama/custom_ollama.yaml
